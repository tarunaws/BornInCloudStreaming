#This deployment is only for handling small files of 3 minutes.
#Node label
#kubectl label nodes ip-10-10-100-239 controlplane=core1
#kubectl label nodes ip-10-10-100-239 controlplane=core2
#kubeadm token create --print-join-command --ttl 0
#instance type : c6a.12xlarge , vCPU :48 , RAM : 96GB
####### Default resource limit ########
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: "2000m"
      memory: "4000Mi"
    defaultRequest:
      cpu: "1000m"
      memory: "1000Mi"
    type: Container

---
#######Service Account ##############
apiVersion: v1
kind: ServiceAccount
metadata:
  name: compress-job-sa

---
###### Roles ############
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: compress-job-role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs","jobs/status"]
  verbs: ["create", "delete","get", "watch", "list"]

---
#########Role Binding ##########
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: compress-job-rb
  namespace: default
subjects:
- kind: ServiceAccount
  name: compress-job-sa
  namespace: default
roleRef:
  kind: Role
  name: compress-job-role
  apiGroup: rbac.authorization.k8s.io

---
#submit
apiVersion: v1
kind: Pod
metadata:
   name: submit
   labels:
      app: core
spec:
  nodeSelector:
    controlplane: "core1"
  containers:
  - name: submit-1
    image: tarunaws/psl:PSL_v17
    imagePullPolicy: Always
    command: ["/bin/sh", "-c"]
    args:
      - echo starting;
        while true;
        do
        sleep 2;
        python3 1_submit.py;
        date;
        done;
        echo done;
    resources:
      requests:
        cpu: "1000m"
        memory: "1000Mi"
      limits:
        cpu: "2000m"
        memory: "2000Mi"
    volumeMounts:
    - name: cp-media
      mountPath: /media
  volumes:
  - name: cp-media
    hostPath:
      path: /mnt/s3bucket
  serviceAccountName: compress-job-sa

---
#contentid
apiVersion: v1
kind: Pod
metadata:
  name: contentid
  labels:
     app: core
spec:
  nodeSelector:
    controlplane: "core1"
  containers:
  - name: contentid-1
    image: tarunaws/psl:PSL_v17
    imagePullPolicy: Always
    command: ["/bin/sh", "-c"]
    args:
      - echo starting;
        while true;
        do
        sleep 2;
        python3 2_contentid.py;
        date;
        done;
        echo done;
    resources:
      requests:
        cpu: "1000m"
        memory: "1000Mi"
      limits:
        cpu: "2000m"
        memory: "2000Mi"
    volumeMounts:
    - name: cp-media
      mountPath: /media
  volumes:
  - name: cp-media
    hostPath:
      path: /mnt/s3bucket
  serviceAccountName: compress-job-sa

---
#Splitter
apiVersion: v1
kind: Pod
metadata:
  name: splitter
  labels:
     app: core
spec:
  nodeSelector:
    controlplane: "core1"
  containers:
  - name: splitter-1
    image: tarunaws/psl:PSL_v17
    imagePullPolicy: Always
    command: ["/bin/sh", "-c"]
    args:
    - echo starting;
      while true;
      do
      sleep 2;
      python3 3_splitter.py;
      date;
      done;
      echo done;
    resources:
      requests:
        cpu: "12000m"
        memory: "32000Mi"
      limits:
        cpu: "20000m"
        memory: "42000Mi"
    volumeMounts:
    - name: cp-media
      mountPath: /media
  volumes:
  - name: cp-media
    hostPath:
      path: /mnt/s3bucket
  serviceAccountName: compress-job-sa

---
#Transcoder
apiVersion: v1
kind: Pod
metadata:
  name: transcoder
  labels:
     app: core
spec:
  nodeSelector:
    controlplane: "core1"
  containers:
  - name: transcoder-1
    image: tarunaws/psl:PSL_v17
    imagePullPolicy: Always
    command: ["/bin/sh", "-c"]
    args:
    - echo starting;
      while true;
      do
      python3 4_transcoder.py;
      date;
      done;
      echo done;
    resources:
      requests:
        cpu: "4000m"
        memory: "4000Mi"
      limits:
        cpu: "6000m"
        memory: "6000Mi"
    volumeMounts:
    - name: cp-media
      mountPath: /media
  volumes:
  - name: cp-media
    hostPath:
      path: /mnt/s3bucket
  serviceAccountName: compress-job-sa

---
#Joining
apiVersion: v1
kind: Pod
metadata:
  name: joiner
  labels:
     app: core
spec:
  nodeSelector:
    controlplane: "core1"
  containers:
  - name: joiner-1
    image: tarunaws/psl:PSL_v17
    imagePullPolicy: Always
    command: ["/bin/sh", "-c"]
    args:
    - echo starting;
      while true;
      do
      sleep 2;
      python3 5_joining.py;
      date;
      done;
      echo done;
    resources:
      requests:
        cpu: "12000m"
        memory: "24000Mi"
      limits:
        cpu: "20000m"
        memory: "28000Mi"
    volumeMounts:
    - name: cp-media
      mountPath: /media
  volumes:
  - name: cp-media
    hostPath:
        path: /mnt/s3bucket
  serviceAccountName: compress-job-sa

---
#Packager
apiVersion: v1
kind: Pod
metadata:
  name: packager
  labels:
     app: core
spec:
  nodeSelector:
    controlplane: "core1"
  containers:
  - name: packager-1
    image: tarunaws/psl:PSL_v17
    imagePullPolicy: Always
    command: ["/bin/sh", "-c"]
    args:
    - echo starting;
      while true;
      do
      sleep 2;
      python3 6_packager.py;
      date;
      done;
      echo done;
    resources:
      requests:
        cpu: "12000m"
        memory: "32000Mi"
      limits:
        cpu: "15000m"
        memory: "40000Mi"
    volumeMounts:
    - name: cp-media
      mountPath: /media
  volumes:
  - name: cp-media
    hostPath:
      path: /mnt/s3bucket
  serviceAccountName: compress-job-sa

---
#Cronjob for deleting completed jobs
apiVersion: batch/v1
kind: CronJob
metadata:
  name: my-cronjob
spec:
  schedule: "*/15 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: compress-job-sa
          containers:
          - name: kubectl-container
            image: bitnami/kubectl:latest
            command: ["sh", "-c", "kubectl delete jobs --field-selector status.successful=1"]
          restartPolicy: Never
